{
  "metadata": {
    "name": "Llama 3.2 WebGPU Worker",
    "language_info": {
      "name": "JavaScript",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "// Import the transformers.js library\nconst { AutoTokenizer, AutoModelForCausalLM, TextStreamer, InterruptableStoppingCriteria } = await import('https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.3.3');\n\n// Check for WebGPU support\nasync function checkWebGPU() {\n  try {\n    const adapter = await navigator.gpu.requestAdapter();\n    if (!adapter) {\n      throw new Error(\"WebGPU is not supported (no adapter found)\");\n    }\n    return true;\n  } catch (e) {\n    console.error(e.toString());\n    return false;\n  }\n}\n\n// Wait for WebGPU check\nconst webGPUSupported = await checkWebGPU();\nconsole.log(`WebGPU supported: ${webGPUSupported}`);\n\nif (!webGPUSupported) {\n  throw new Error(\"This notebook requires WebGPU support\");\n}",
      "status": "[-]",
      "output": "",
      "type": "code"
    },
    {
      "code": "/**\n * This class uses the Singleton pattern to enable lazy-loading of the pipeline\n */\nclass TextGenerationPipeline {\n  static model_id = \"onnx-community/Llama-3.2-1B-Instruct-q4f16\";\n  static tokenizer;\n  static model;\n\n  static async getInstance(progress_callback = null) {\n    this.tokenizer ??= await AutoTokenizer.from_pretrained(this.model_id, {\n      progress_callback,\n    });\n\n    this.model ??= await AutoModelForCausalLM.from_pretrained(this.model_id, {\n      dtype: \"q4f16\",\n      device: \"webgpu\",\n      progress_callback,\n    });\n\n    return [this.tokenizer, this.model];\n  }\n}\n\n// Create a stopping criteria that can be interrupted\nconst stopping_criteria = new InterruptableStoppingCriteria();\n\n// Initialize the pipeline and display loading progress\nconsole.log(\"Loading model...\");\nconst [tokenizer, model] = await TextGenerationPipeline.getInstance((progress) => {\n  console.log(`Loading: ${progress.file} - ${Math.round(progress.progress * 100)}%`);\n});\n\nconsole.log(\"Compiling shaders and warming up model...\");\n\n// Run model with dummy input to compile shaders\nconst dummyInputs = tokenizer(\"a\");\nawait model.generate({ ...dummyInputs, max_new_tokens: 1 });\n\nconsole.log(\"Model ready!\");",
      "status": "[-]",
      "output": "",
      "type": "code"
    },
    {
      "code": "// Function to generate text based on messages\nasync function generate(messages) {\n  console.log(\"Generating response...\");\n  \n  const inputs = tokenizer.apply_chat_template(messages, {\n    add_generation_prompt: true,\n    return_dict: true,\n  });\n\n  let startTime;\n  let numTokens = 0;\n  let tps;\n  \n  // Track generation stats\n  const token_callback_function = () => {\n    startTime ??= performance.now();\n\n    if (numTokens++ > 0) {\n      tps = (numTokens / (performance.now() - startTime)) * 1000;\n      if (numTokens % 10 === 0) {\n        console.log(`Generated ${numTokens} tokens at ${tps.toFixed(2)} tokens/sec`);\n      }\n    }\n  };\n  \n  // Output each token as it's generated\n  const callback_function = (output) => {\n    // In a notebook environment, we'll just log the output\n    process.stdout.write(output);\n  };\n\n  const streamer = new TextStreamer(tokenizer, {\n    skip_prompt: true,\n    skip_special_tokens: true,\n    callback_function,\n    token_callback_function,\n  });\n\n  console.log(\"Starting generation...\");\n  \n  const { sequences } = await model.generate({\n    ...inputs,\n    do_sample: false,\n    max_new_tokens: 1024,\n    streamer,\n    stopping_criteria,\n    return_dict_in_generate: true,\n  });\n\n  const decoded = tokenizer.batch_decode(sequences, {\n    skip_special_tokens: true,\n  });\n\n  console.log(\"\\nGeneration complete!\");\n  return decoded;\n}\n\n// Example chat messages\nconst messages = [\n  { role: \"system\", content: \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\" },\n  { role: \"user\", content: \"What is the theory of relativity?\" }\n];\n\n// Generate and display the response\nconst response = await generate(messages);\nconsole.log(\"\\nFinal response:\");\nconsole.log(response);",
      "status": "[-]",
      "output": "",
      "type": "code"
    },
    {
      "code": "// Interactive chat function\nasync function chat(userMessage) {\n  // Add user message to conversation\n  messages.push({ role: \"user\", content: userMessage });\n  \n  // Generate response\n  const response = await generate(messages);\n  \n  // Add assistant response to conversation\n  messages.push({ role: \"assistant\", content: response[0] });\n  \n  return response[0];\n}\n\n// Example of using the chat function\n// Uncomment and modify to use:\n// const userQuestion = \"How does quantum mechanics relate to relativity?\";\n// const assistantResponse = await chat(userQuestion);\n// console.log(\"User:\", userQuestion);\n// console.log(\"Assistant:\", assistantResponse);",
      "status": "[-]",
      "output": "",
      "type": "code"
    }
  ],
  "source": "https://github.com/gopi-suvanam/scribbler",
  "run_on_load": false
}
