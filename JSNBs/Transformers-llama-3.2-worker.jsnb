{
  "metadata": {
    "name": "Llama 3.2 WebGPU Simple",
    "language_info": {
      "name": "JavaScript",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "// Import the transformers.js library\nconst { AutoTokenizer, AutoModelForCausalLM, TextStreamer } = await import('https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.3.3');\n\n// Check for WebGPU support\nconst adapter = await navigator.gpu?.requestAdapter();\nif (!adapter) {\n  throw new Error(\"WebGPU is not supported (no adapter found)\");\n}\nconsole.log(\"WebGPU is supported!\");\n\n// Model ID\nconst model_id = \"onnx-community/Llama-3.2-1B-Instruct-q4f16\";\n\n// Load tokenizer and model\nconsole.log(\"Loading model...\");\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\nconst model = await AutoModelForCausalLM.from_pretrained(model_id, {\n  dtype: \"q4f16\",\n  device: \"webgpu\"\n});\nconsole.log(\"Model loaded successfully!\");",
      "status": "[-]",
      "output": "",
      "type": "code"
    },
    {
      "code": "// Define your prompt\nconst messages = [\n  { role: \"system\", content: \"You are a helpful assistant.\" },\n  { role: \"user\", content: \"What is the theory of relativity?\" }\n];\n\n// Prepare input with chat template\nconst inputs = tokenizer.apply_chat_template(messages, {\n  add_generation_prompt: true,\n  return_dict: true\n});\n\n// Set up the streamer for token-by-token output\nconst streamer = new TextStreamer(tokenizer, {\n  skip_prompt: true,\n  skip_special_tokens: true\n});\n\n// Generate text\nconsole.log(\"Generating response...\");\nconst output = await model.generate({\n  ...inputs,\n  max_new_tokens: 512,\n  streamer\n});\n\n// Display the complete output\nconst decoded = tokenizer.batch_decode(output, {\n  skip_special_tokens: true\n});\nconsole.log(\"\\nComplete response:\");\nconsole.log(decoded);",
      "status": "[-]",
      "output": "",
      "type": "code"
    }
  ],
  "source": "https://github.com/EXTREMOPHILARUM/scribbler-notebooks",
  "run_on_load": false
}
