{
  "metadata": {
    "name": "Llama 3.2 WebGPU Simple",
    "language_info": {
      "name": "JavaScript",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "// Import the transformers.js library\nconst { AutoTokenizer, AutoModelForCausalLM } = await import('https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.3.3');\n\n// Check for WebGPU support\nconst adapter = await navigator.gpu?.requestAdapter();\nif (!adapter) {\n  throw new Error(\"WebGPU is not supported (no adapter found)\");\n}\nconsole.log(\"WebGPU is supported!\");\n\n// Model ID\nconst model_id = \"onnx-community/Llama-3.2-1B-Instruct-q4f16\";\n\n// Load tokenizer and model\nconsole.log(\"Loading model...\");\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\nconst model = await AutoModelForCausalLM.from_pretrained(model_id, {\n  dtype: \"q4f16\",\n  device: \"webgpu\"\n});\n\n// Store in window for access across cells\nwindow.tokenizer = tokenizer;\nwindow.model = model;\n\nconsole.log(\"Model loaded successfully!\");",
      "status": "[-]",
      "output": "",
      "type": "code"
    },
    {
      "code": "// Define your prompt\nconst messages = [\n  { role: \"system\", content: \"You are a helpful assistant.\" },\n  { role: \"user\", content: \"What is the theory of relativity?\" }\n];\n\n// Prepare input with chat template\nconst inputs = window.tokenizer.apply_chat_template(messages, {\n  add_generation_prompt: true,\n  return_dict: true\n});\n\n// Generate text\nconsole.log(\"Generating response...\");\nconst output = await window.model.generate({\n  ...inputs,\n  max_new_tokens: 512\n});\n\n// Display the complete output\nconst decoded = window.tokenizer.batch_decode(output, {\n  skip_special_tokens: true\n});\n\n// Extract the assistant's response\nlet assistantResponse = \"\";\nif (decoded && decoded.length > 0) {\n  // The response comes as a string that contains \"assistant\" followed by the actual response\n  const fullText = decoded[0];\n  const assistantMarker = \"assistant\\n\\n\";\n  const assistantIndex = fullText.lastIndexOf(assistantMarker);\n  \n  if (assistantIndex !== -1) {\n    // Extract just the assistant's response\n    assistantResponse = fullText.substring(assistantIndex + assistantMarker.length);\n  } else {\n    // Fallback if the format is different\n    assistantResponse = fullText;\n  }\n}\n\n// Store in window for access in next cell\nwindow.assistantResponse = assistantResponse;\n\n// Display the raw formatted response\nconsole.log(\"\\nFormatted response:\");\nconsole.log(assistantResponse);",
      "status": "[-]",
      "output": "",
      "type": "code"
    },
    {
      "code": "// Create a formatted HTML output for better readability\nconst formattedHTML = `<div style=\"font-family: system-ui; line-height: 1.5; padding: 1rem; max-width: 800px; margin: 0 auto;\">\n  <h3>Response:</h3>\n  <div style=\"white-space: pre-wrap; background: #f5f5f5; padding: 1rem; border-radius: 8px; border: 1px solid #ddd;\">\n    ${window.assistantResponse.replace(/\\*\\*(.*?)\\*\\*/g, '<strong>$1</strong>')}\n  </div>\n</div>`;\n\n// Return the HTML to be displayed in the cell's output\nformattedHTML;",
      "status": "[-]",
      "output": "",
      "type": "code"
    }
  ],
  "source": "https://github.com/EXTREMOPHILARUM/scribbler-notebooks",
  "run_on_load": false
}
