{
  "metadata": {
    "name": "janus-pro",
    "language_info": {
      "name": "JavaScipt",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "// Dynamically import the ONNX Runtime Web library\nconst ort = await import('https://cdn.jsdelivr.net/npm/onnxruntime-web@1.20.1');\n\nscrib.show(ort)\nconst session = ort.InferenceSession.create(\n      'https://huggingface.co/onnx-community/Janus-Pro-1B-ONNX/resolve/main/model.onnx',\n      {\n        executionProviders: ['webgpu','wasm'], \n      }\n    );",
      "status": "",
      "output": "<p>// Dynamically import the ONNX Runtime Web library\nconst ort = await import('<a href=\"https://cdn.jsdelivr.net/npm/onnxruntime-web@1.20.1\">https://cdn.jsdelivr.net/npm/onnxruntime-web@1.20.1</a>');</p>\n<p>scrib.show(ort)\nconst session = ort.InferenceSession.create(\n      '<a href=\"https://huggingface.co/onnx-community/Janus-Pro-1B-ONNX/resolve/main/model.onnx\">https://huggingface.co/onnx-community/Janus-Pro-1B-ONNX/resolve/main/model.onnx</a>',\n      {\n        executionProviders: ['webgpu','wasm'], \n      }\n    );</p>\n",
      "type": "html"
    },
    {
      "code": "const { AutoTokenizer, AutoModelForCausalLM } = await import('https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.3.3');\nconst modelId = 'HuggingFaceTB/SmolLM2-360M-Instruct';\ntokenizer = await AutoTokenizer.from_pretrained(modelId);\nmodel = await AutoModelForCausalLM.from_pretrained(modelId, {\n\tquantized: true, // Use the quantized model for better performance\n    device: 'webgpu', // Use 'webgpu' for GPU acceleration; fallback to 'wasm' if not supported\n  \tdtype: 'q4'\n\t});\n",
      "status": "[2]<br><span style=\"font-size:8px\">11.937s<span></span></span>",
      "output": "",
      "type": "code"
    },
    {
      "code": "",
      "status": "",
      "output": "",
      "type": "code"
    },
    {
      "code": "const prompt = \"Write a 100-word article on the benefits of open-source in AI research.\";\n\nconst inputIds = tokenizer.encode(prompt, { addSpecialTokens: true });\nconst output = await model.generate(inputIds, {\n\tmaxLength: 100, // Adjust the max length as needed\n    temperature: 0.7, // Adjust the temperature for creativity\n    top_p: 0.9, // Adjust the top_p for nucleus sampling\n    do_sample: true,\n  });\nconst generatedText = tokenizer.decode(output, { skipSpecialTokens: true });\nconsole.log('Generated text:', generatedText);",
      "status": "[-]",
      "output": "<p class=\"error\">Cannot read properties of null (reading 'dims')</p>",
      "type": "code"
    }
  ],
  "source": "https://github.com/gopi-suvanam/scribbler",
  "run_on_load": false
}